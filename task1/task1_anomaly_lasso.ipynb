{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy as copy\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import mixture\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_selection(X_ls,y_ls,alpha,n):\n",
    "    preprocess = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), \n",
    "                               #PCA(n_components=X_ls.shape[1], whiten=True), \n",
    "                               StandardScaler())\n",
    "    X_pre = preprocess.fit_transform(X_ls)\n",
    "    Y_pre = StandardScaler().fit_transform(y_ls)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=100000)\n",
    "    lasso.fit(X_pre, Y_pre) \n",
    "    lasso_coefs = lasso.coef_\n",
    "    coef = pd.DataFrame(lasso_coefs, index=X_ls.columns)\n",
    "    best_col = abs(coef).nlargest(n=n,columns=0).index\n",
    "    return best_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(X_train_od, y_train_od, n_PCA, quantile):\n",
    "    \n",
    "    # project using PCA\n",
    "    clf = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), \n",
    "                        StandardScaler(), \n",
    "                        PCA(n_components=n_PCA, whiten=False))\n",
    "    # density estimation with GMM\n",
    "    gmm = mixture.GaussianMixture(n_components=1, covariance_type='full')\n",
    "\n",
    "    anomaly_detected = True\n",
    "    iter = 0\n",
    "    cutoff=np.nan\n",
    "    anom_id=set()\n",
    "    df_total_od=pd.concat([X_train_od,y_train_od], axis=1)\n",
    "    \n",
    "    # method : estimate density (but contaminated with outliers), remove outliers, \n",
    "    #          re-estimate density without outliers, remove (newly detected) outliers,\n",
    "    #          ...\n",
    "    # loop until no more outliers are detected\n",
    "    while anomaly_detected:\n",
    "        iter+=1\n",
    "        \n",
    "        # extract features using PCA\n",
    "        feat = pd.DataFrame(clf.fit_transform(df_total_od), index=df_total_od.index)\n",
    "        \n",
    "        # compute log-likelihood of observed data\n",
    "        gmm.fit(feat)\n",
    "        loglike = gmm.score_samples(feat)\n",
    "        \n",
    "        # set the cutoff at the first iteration\n",
    "        if iter==1:\n",
    "            cutoff = np.quantile(loglike, quantile)\n",
    "        \n",
    "        # add newly detected anomalies to the set of all anomalies\n",
    "        anom_id=anom_id.union(set(df_total_od.index[(loglike <= cutoff)]))\n",
    "        \n",
    "        # number of newly detected outliers\n",
    "        n_anom = df_total_od.shape[0] - (loglike > cutoff).sum()\n",
    "        if n_anom==0:\n",
    "            anomaly_detected = False\n",
    "        #print(\"Number of removed outliers: \", n_anom)\n",
    "        \n",
    "        # remove newly detected outliers\n",
    "        df_total_od=df_total_od[loglike > cutoff]\n",
    "        \n",
    "        \n",
    "    return anom_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(X_train, X_test, Y_train, alpha, lasso_nlargest, n_PCA_anom, quantile_anom):\n",
    "    max_iter=5\n",
    "    X=copy.copy(X_train)\n",
    "    Y=copy.copy(Y_train)\n",
    "    \n",
    "    # idea : select best features, remove outliers (or vice-versa)\n",
    "    # problem : outliers could influence the selection of best features and then make the anomaly detection more difficult\n",
    "    # solution : loop on feature selection and anomaly detection to remove influence of outliers on feature selection\n",
    "    for i in range(max_iter):\n",
    "        # select best features (based on data contaminated by outliers during first round, then without outliers in following runs)\n",
    "        col = lasso_selection(X,Y,alpha,lasso_nlargest)\n",
    "        X_train_red = X_train[X_train.columns.intersection(col)]\n",
    "        \n",
    "        # detect outliers based on features\n",
    "        anom_id = outlier_detection(X_train_red, Y_train, n_PCA=n_PCA_anom, quantile=quantile_anom)\n",
    "        \n",
    "        # remove outliers from original data\n",
    "        X=copy.copy(X_train)\n",
    "        Y=copy.copy(Y_train)\n",
    "        for id in anom_id:\n",
    "            X = X.drop(id)\n",
    "            Y = Y.drop(id)\n",
    "        \n",
    "        # stop if no change \n",
    "        if (i>0 and set(col_prev)==set(col) and anom_id_prev==anom_id):\n",
    "            break\n",
    "        col_prev = col\n",
    "        anom_id_prev = anom_id\n",
    "    # print(f'Number of anomalies detected: {len(anom_id)}')\n",
    "    \n",
    "    # once outliers are removed, we select best features\n",
    "    col = lasso_selection(X,Y,alpha,lasso_nlargest)\n",
    "    X_train_proc = X[X.columns.intersection(col)]\n",
    "    Y_train_proc = Y\n",
    "    X_test_proc = X_test[X_test.columns.intersection(col)]\n",
    "    \n",
    "    # we have removed outliers and selected best features but haven't done the final imputation yet\n",
    "    return X_train_proc, X_test_proc, Y_train_proc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv').drop(columns=\"id\")\n",
    "X_test = pd.read_csv('X_test.csv').drop(columns=\"id\")\n",
    "\n",
    "Y_train = pd.read_csv('Y_train.csv').drop(columns=\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, X_train.var() != 0.0]\n",
    "X_test = X_test.loc[:, X_test.var() != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = copy.copy(X_train)\n",
    "Y_train_t = copy.copy(Y_train)\n",
    "mask = np.arange(X_train_t.shape[0])\n",
    "np.random.shuffle(mask)\n",
    "X_train_t = X_train_t.loc[mask]\n",
    "Y_train_t = Y_train_t.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\t\tGBR\t\tLGB\n",
      "0.010\t\t0.4674\t\t0.4753\n",
      "0.020\t\t0.4317\t\t0.4431\n",
      "0.030\t\t0.4544\t\t0.4470\n",
      "0.040\t\t0.4462\t\t0.4539\n",
      "0.050\t\t0.4187\t\t0.4279\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0/50.0 #50 #22\n",
    "lasso_nlargest = 20 # 30 #50\n",
    "n_PCA_anom = 20 #23\n",
    "quantile_anom = 0.01\n",
    "n_KNN = 6\n",
    "\n",
    "print(f'param\\t\\tGBR\\t\\tLGB')\n",
    "\n",
    "for param in np.linspace(0.01, 0.05, 5):\n",
    "    quantile_anom = param\n",
    "\n",
    "    step = X_train_t.shape[0] // 5\n",
    "    r2_val_lgb, r2_train_lgb = 0, 0\n",
    "    r2_val_gbr, r2_train_gbr = 0, 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        X_val, y_val = X_train_t.iloc[i*step:(i+1)*step], Y_train_t.iloc[i*step:(i+1)*step]\n",
    "        X_train_CV = pd.concat((X_train_t.iloc[(i+1)*step:], X_train_t.iloc[:i*step]), axis=0)\n",
    "        Y_train_CV = pd.concat((Y_train_t.iloc[(i+1)*step:], Y_train_t.iloc[:i*step]), axis=0)\n",
    "        \n",
    "        X_train_CV, X_val_proc, y_train_CV = process(X_train_CV, X_val, Y_train_CV, alpha, lasso_nlargest, int(n_PCA_anom), quantile_anom)\n",
    "        \n",
    "        # imputation \n",
    "        clf = make_pipeline(StandardScaler(), \n",
    "                            KNNImputer(missing_values=np.nan, n_neighbors=n_KNN), \n",
    "                            StandardScaler(), \n",
    "                            #PCA(n_components=int(n_PCA_feat), whiten=False),\n",
    "                            )\n",
    "        clf.fit(pd.concat([X_train_CV, X_val_proc], axis=0))\n",
    "        X_train_CV = clf.transform(X_train_CV)\n",
    "        X_val = clf.transform(X_val_proc)\n",
    "        y_scaler = StandardScaler()\n",
    "        y_train_CV = y_scaler.fit_transform(y_train_CV).ravel()\n",
    "        \n",
    "        lgb = lightgbm.LGBMRegressor(max_depth=2, learning_rate=0.015, n_estimators=500)\n",
    "        gbr = GradientBoostingRegressor(max_depth=2, learning_rate=0.015, n_estimators=500)\n",
    "        \n",
    "        gbr.fit(X_train_CV, y_train_CV)\n",
    "        lgb.fit(X_train_CV, y_train_CV)\n",
    "        \n",
    "        r2_val_lgb += r2_score(y_val, y_scaler.inverse_transform(lgb.predict(X_val).reshape(-1, 1)))\n",
    "        r2_val_gbr += r2_score(y_val, y_scaler.inverse_transform(gbr.predict(X_val).reshape(-1, 1)))\n",
    "        \n",
    "        r2_train_lgb += r2_score(y_train_CV, lgb.predict(X_train_CV))\n",
    "        r2_train_gbr += r2_score(y_train_CV, gbr.predict(X_train_CV))\n",
    "        \n",
    "    print(f'{param:5.3f}\\t\\t{r2_val_gbr/5:6.4f}\\t\\t{r2_val_lgb/5:6.4f}')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 15:17:23) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b72a1a9fcdc4a37b876d59b8fde098d6bda68dcbb49ecf7e0339451022590265"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
