{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)\n",
    "        \n",
    "def flatten(dicts):\n",
    "    \n",
    "    # extract the annotated video frames, their labels\n",
    "    # and the boxes\n",
    "    \n",
    "    images = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, dic in enumerate(dicts):\n",
    "        \n",
    "        video = dic['video']\n",
    "        frames = dic['frames']\n",
    "        dic_labels = dic['label']\n",
    "        \n",
    "        images.append((video[:,:,frames[0]] / 255).astype('float32'))\n",
    "        images.append((video[:,:,frames[1]] / 255).astype('float32'))\n",
    "        images.append((video[:,:,frames[2]] / 255).astype('float32'))\n",
    "        \n",
    "        labels.append(dic_labels[:,:,frames[0]].astype('uint8'))\n",
    "        labels.append(dic_labels[:,:,frames[1]].astype('uint8'))\n",
    "        labels.append(dic_labels[:,:,frames[2]].astype('uint8'))\n",
    "        \n",
    "        boxes.append(dic['box'].astype('uint8'))\n",
    "        boxes.append(dic['box'].astype('uint8'))\n",
    "        boxes.append(dic['box'].astype('uint8'))\n",
    "        \n",
    "    return images, boxes, labels\n",
    "\n",
    "def resize(images, boxes, labels, size):\n",
    "    \n",
    "    # resize images, boxes and labels\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        \n",
    "        images[i] = cv2.resize(images[i], size, interpolation = cv2.INTER_LANCZOS4)\n",
    "        boxes[i] = cv2.resize(boxes[i], size, interpolation = cv2.INTER_LANCZOS4)\n",
    "        labels[i] = cv2.resize(labels[i], size, interpolation = cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # add number of channels (in this case 1) at the front for the\n",
    "        # right input shape for the pytorch layers\n",
    "        \n",
    "        images[i] = np.expand_dims(images[i], axis=0)\n",
    "        images[i] = np.expand_dims(images[i], axis=0)\n",
    "        \n",
    "    return np.concatenate(images, axis=0), boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = load_zipped_pickle(\"data/train.pkl\")\n",
    "test_data = load_zipped_pickle(\"data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, boxes, labels = flatten(train_data)\n",
    "images, boxes, labels = resize(images, boxes, labels, size)\n",
    "\n",
    "# TODO: add data augmentation, denoising, other preprocessing steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_enc_1 = self.block(1, 64, True)\n",
    "        self.block_enc_2 = self.block(64, 128, True)\n",
    "        self.block_enc_3 = self.block(128, 256, True)\n",
    "        self.block_enc_4 = self.block(256, 512, True)\n",
    "        \n",
    "        #self.block_inbetween = self.block(512, 1024, False)\n",
    "        \n",
    "        #self.block_dec_1 = self.block(1024, 512, False)\n",
    "        #self.block_dec_2 = self.block(512, 256, False)\n",
    "        #self.block_dec_3 = self.block(256, 128, False)\n",
    "        #self.block_dec_4 = self.block(128, 64, False)\n",
    "        \n",
    "    def block(self, channels, filters, enc):\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        modules.append(nn.Conv2d(channels, filters, 3, 1))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Conv2d(filters, filters, 3, 1))\n",
    "        modules.append(nn.ReLU())\n",
    "\n",
    "        # if it is an encoder block\n",
    "        # then max pool, else upconv\n",
    "        \n",
    "        if enc:\n",
    "            modules.append(nn.MaxPool2d(2))\n",
    "            \n",
    "        return nn.Sequential(*modules)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.block_enc_1(x)\n",
    "        x = self.block_enc_2(x)\n",
    "        x = self.block_enc_3(x)\n",
    "        x = self.block_enc_4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet().to(device)\n",
    "train_images = torch.from_numpy(images[:10]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 4, 4])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet(train_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "586f7db3c72cdbd2111a5fe733ffa252a59827d1d7df0e37a9fca40f971001cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
